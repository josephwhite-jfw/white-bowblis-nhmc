{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9a467a-4dad-41c5-82f0-4158b7b9007d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m PROJECT_ROOT \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# works if notebook lives in /notebooks\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RAW_DIR\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAW_DIR is\u001b[39m\u001b[38;5;124m\"\u001b[39m, RAW_DIR)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Tell Python where the project's \"src\" folder is\n",
    "import sys, pathlib\n",
    "\n",
    "PROJECT_ROOT = pathlib.Path.cwd().parents[1]   # works if notebook lives in /notebooks\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from src.utils.paths import RAW_DIR\n",
    "print(\"RAW_DIR is\", RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bce25a-57d1-4471-9991-d3b8ad202eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from io import TextIOWrapper\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm.autonotebook import tqdm as _tqdm\n",
    "\n",
    "    def tqdm(it, **kw):\n",
    "        return _tqdm(it, **kw)\n",
    "except ModuleNotFoundError:\n",
    "    tqdm = lambda it, **kw: it  # type: ignore[misc]\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# Repo paths\n",
    "# ------------------------------------------------------------------ #\n",
    "try:\n",
    "    from src.utils.paths import RAW_DIR, INTERIM_DIR\n",
    "except ModuleNotFoundError:\n",
    "    ROOT = Path(__file__).resolve().parents[2]\n",
    "    RAW_DIR = ROOT / \"data\" / \"raw\"\n",
    "    INTERIM_DIR = ROOT / \"data\" / \"interim\"\n",
    "\n",
    "ZIP_DIR = RAW_DIR / \"nh-compare\"\n",
    "OUT_PARQUET = INTERIM_DIR / \"ownership_combined.parquet\"\n",
    "OUT_CSV = OUT_PARQUET.with_suffix(\".csv\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# Column-standardisation map (same as your notebook)\n",
    "# ------------------------------------------------------------------ #\n",
    "COLUMN_MAP: Dict[str, str] = {\n",
    "    # provider id\n",
    "    \"cms certification number (ccn)\": \"cms_certification_number\",\n",
    "    \"federal provider number\": \"cms_certification_number\",\n",
    "    \"provnum\": \"cms_certification_number\",\n",
    "    # provider name\n",
    "    \"provider name\": \"provider_name\",\n",
    "    \"provname\": \"provider_name\",\n",
    "    # role\n",
    "    \"role played by owner or manager in facility\": \"role\",\n",
    "    \"role played by owner in facility\": \"role\",\n",
    "    \"role of owner or manager\": \"role\",\n",
    "    \"owner role\": \"role\",\n",
    "    \"role desc\": \"role\",\n",
    "    # ownership meta\n",
    "    \"owner type\": \"owner_type\",\n",
    "    \"owner name\": \"owner_name\",\n",
    "    \"ownership percentage\": \"ownership_percentage\",\n",
    "    \"owner percentage\": \"ownership_percentage\",\n",
    "    \"association date\": \"association_date\",\n",
    "    # processing date\n",
    "    \"processing date\": \"processing_date\",\n",
    "    \"processingdate\": \"processing_date\",\n",
    "    \"process date\": \"processing_date\",\n",
    "    \"processdate\": \"processing_date\",\n",
    "    \"filedate\": \"processing_date\",\n",
    "}\n",
    "TARGET_COLS = list(set(COLUMN_MAP.values()))\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# Logging\n",
    "# ------------------------------------------------------------------ #\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------ #\n",
    "def _clean_col(raw: str) -> str | None:\n",
    "    raw = re.sub(r\"[^a-z0-9]+\", \" \", raw.lower()).strip()\n",
    "    return COLUMN_MAP.get(raw)\n",
    "\n",
    "\n",
    "def _iter_inner_csvs(zip_files: Iterable[Path], chunksize: int | None) -> Iterable[pd.DataFrame]:\n",
    "    import zipfile\n",
    "\n",
    "    pat_old = re.compile(r\"ownership_download\", re.I)\n",
    "    pat_new = re.compile(r\"NH_Ownership_\", re.I)\n",
    "\n",
    "    for outer in tqdm(zip_files, desc=\"ZIP archives\"):\n",
    "        with zipfile.ZipFile(outer) as z_outer:\n",
    "            for inner_name in z_outer.namelist():\n",
    "                # choose correct owner file inside the inner zip\n",
    "                mm_yyyy = re.search(r\"_(\\d{2})_(\\d{4})\\.zip$\", inner_name)\n",
    "                if not mm_yyyy:\n",
    "                    continue  # ignore scores/inspection zips\n",
    "\n",
    "                month, year = map(int, mm_yyyy.groups())\n",
    "                is_new_fmt = (year > 2020) or (year == 2020 and month >= 8)\n",
    "\n",
    "                if (is_new_fmt and not pat_new.search(inner_name)) or (\n",
    "                    not is_new_fmt and not pat_old.search(inner_name)\n",
    "                ):\n",
    "                    continue  # wrong file inside this inner zip\n",
    "\n",
    "                with z_outer.open(inner_name) as inner_bytes:\n",
    "                    with zipfile.ZipFile(inner_bytes) as z_inner:\n",
    "                        for member in z_inner.namelist():\n",
    "                            if not member.lower().endswith(\".csv\"):\n",
    "                                continue\n",
    "                            logging.info(\"Reading %s in %s\", member, outer.name)\n",
    "                            with z_inner.open(member) as fbytes:\n",
    "                                ftxt: TextIOWrapper = TextIOWrapper(fbytes, encoding=\"utf-8\")\n",
    "                                if chunksize:\n",
    "                                    for chunk in pd.read_csv(\n",
    "                                        ftxt, dtype=str, chunksize=chunksize\n",
    "                                    ):\n",
    "                                        yield chunk, month, year, member\n",
    "                                else:\n",
    "                                    yield pd.read_csv(ftxt, dtype=str), month, year, member\n",
    "\n",
    "\n",
    "def _standardise(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    # map / filter columns\n",
    "    new_cols = {c: _clean_col(c) for c in df_raw.columns}\n",
    "    df = pd.DataFrame()\n",
    "    for old, new in new_cols.items():\n",
    "        if new in TARGET_COLS:\n",
    "            df[new] = df_raw[old]\n",
    "    # add missing empties\n",
    "    for col in TARGET_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df\n",
    "\n",
    "\n",
    "def _fill_ccn(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ccn_col, name_col = \"cms_certification_number\", \"provider_name\"\n",
    "    mapping = (\n",
    "        df.dropna(subset=[ccn_col])\n",
    "        .groupby(name_col, observed=True)[ccn_col]\n",
    "        .agg(lambda s: set(s))\n",
    "    )\n",
    "    unamb = mapping[mapping.str.len() == 1].apply(lambda s: next(iter(s)))\n",
    "    mask = df[ccn_col].isna()\n",
    "    before = mask.sum()\n",
    "    df.loc[mask, ccn_col] = df.loc[mask, name_col].map(unamb)\n",
    "    after = df[ccn_col].isna().sum()\n",
    "    logging.info(\"CCN fill: %s â†’ %s missing (filled %s)\", before, after, before - after)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# Main builders\n",
    "# ------------------------------------------------------------------ #\n",
    "def build_df(chunksize: int | None = None) -> pd.DataFrame:\n",
    "    zip_files = sorted(ZIP_DIR.glob(\"nh_archive_*.zip\"))\n",
    "    if not zip_files:\n",
    "        raise FileNotFoundError(f\"No ZIPs found under {ZIP_DIR}\")\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for raw_df, mm, yyyy, src in _iter_inner_csvs(zip_files, chunksize):\n",
    "        df = _standardise(raw_df)\n",
    "        df[\"source_file\"] = Path(src).stem\n",
    "        df[\"month\"] = mm\n",
    "        df[\"year\"] = yyyy\n",
    "        df[\"date\"] = pd.to_datetime(dict(year=df.year, month=df.month, day=1))\n",
    "        frames.append(df)\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined = combined.sort_values(\"date\").reset_index(drop=True)\n",
    "    combined = _fill_ccn(combined)\n",
    "    logging.info(\"Combined shape %s\", combined.shape)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def write_df(df: pd.DataFrame, to_parquet: bool = True) -> Path:\n",
    "    INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out = OUT_PARQUET if to_parquet else OUT_CSV\n",
    "    if to_parquet:\n",
    "        df.to_parquet(out, index=False)\n",
    "    else:\n",
    "        df.to_csv(out, index=False)\n",
    "    logging.info(\"Wrote %s (%.2f MB)\", out, out.stat().st_size / 1_048_576)\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_and_write_ownership(\n",
    "    chunksize: int | None = None, to_parquet: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    df = build_df(chunksize=chunksize)\n",
    "    write_df(df, to_parquet=to_parquet)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# CLI\n",
    "# ------------------------------------------------------------------ #\n",
    "def _args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Build ownership_combined table.\")\n",
    "    p.add_argument(\"--csv\", action=\"store_true\", help=\"write CSV instead of Parquet\")\n",
    "    p.add_argument(\"--chunksize\", type=int, default=None, help=\"rows per chunk\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    a = _args()\n",
    "    build_and_write_ownership(chunksize=a.chunksize, to_parquet=not a.csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
